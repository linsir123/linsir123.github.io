---
layout: post
title:  "[Nginx] 日志汇总合并"
categories: [default]
---

### 单机
------------------------------

在项目开发过程中由于各种数据分析需求，经常需要服务端提供接口用于采集日志以便用于统计分析。


此处主要介绍有关日志采集的方案，至于数据分析方案可结合项目的大小分别采用如：`php | shell | hadoop`。
日志采集接口在功能上只需要保证正常记录客户端发送上来的日志即可，另外在满足功能的前提下尽量会选择并发量高的服务。
Nginx在处理静态文件的时候效率据说比Apache高好几倍，我们设计一个静态的url让客户端面将日志以query的形式上传至服务器（如：http://xxx.com/log.js?a=1&b=c）。
这样通过Nginx的访问日志我们就实现了采集日志的需求。 


### 集群
------------------------------

当日志请求量处于相对较小的情况下上述方案基本可以应付到。
而当请求量较大时达到Nginx本身的处理瓶颈时则可能需要添加多个Nginx实例进行负载均衡，而添加完实例后我们会大面临一个问题就是如何将日志进行有效地汇总。
通常情况下面我们可能会想到一个方案就是将各实例产生的日志，通过同步或下载复制至主节点，此处我们先不说这个方案的好坏。
引入另一个方案，借由`Scribe`进行日志的汇总，简单说下原理就是用`tail`命令监控日志文件的写入，当文件有增量时将增量日志写入本地的Scribe。

附件为监测脚本：[scribe_nginx_log.py](/public/python/scribe_nginx_log.py){:target="_blank"}


### P's
------------------------------

目前有尝试过`Flume`可以替代`Scribe`，他在`数据源获取`和`负载均衡`方面会比Scribe来得更优。

```
Flume数据源以及输出方式:
Flume提供了从console(控制台)、RPC(Thrift-RPC)、text(文件)、tail(UNIX tail)、syslog(syslog日志系统，支持TCP和UDP等2种模式)，exec(命令执行)等数据源上收集数据的能力。
```
